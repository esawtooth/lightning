Perfect. I’ll now refine the Context Hub design to act as the core system-of-record for Lightning OS: a multi-agent, API-accessed, snapshotting CRDT-based context store with full support for shared editing, collaboration across users and agents, and permissioned access to structured and unstructured data.

I’ll get back to you shortly with a detailed, production-aligned design spec.


# Context Hub – System Design Specification

## Overview & Goals

The **Context Hub** is the authoritative memory and context store for the Lightning/Vextir OS. It holds all agent knowledge, planning state, and user context in a persistent, collaborative knowledge base. External AI agents (and UIs) interact with the Context Hub through network APIs rather than direct memory access, ensuring loose coupling and independent scalability. This design prioritizes concurrent multi-agent writes, multi-user collaboration, auditability, and robust recovery. In line with the product vision, the Context Hub **aggregates and maintains user data from diverse sources** and **enables historical and real-time context-sensitive actions**. Key goals include:

* **High-Concurrency Collaboration:** Multiple agents (potentially for different users) can concurrently update shared documents without conflicts.
* **Structured Knowledge Organization:** Data is arranged in a hierarchical folder tree, each with an *Index Guide* to help agents understand and use the content appropriately.
* **Resilience and Audit:** Regular snapshots to an append-only store (like Git) provide recovery points and an auditable log of context changes.
* **Permission Control:** Fine-grained access controls ensure each user/agent only sees or edits authorized documents, while still enabling intentional sharing.
* **Rich Query Interface:** Both keyword search and semantic vector search are supported for efficient recall of relevant information across the knowledge base.

## System Architecture

&#x20;*Figure: High-level architecture of the Context Hub. Agents (for different users) and user interfaces connect via a REST/WebSocket API to a central Context Hub service. Internally, the hub manages live collaborative documents (CRDT shards) organized into a folder/index hierarchy, enforced by a permission manager. A search index (text and vector) and pointer resolver interact with external stores (snapshot repository, blob storage for files, code repositories, and a vector DB for embeddings).*

The Context Hub comprises a centralized service with modular components for collaboration, storage, indexing, and security. **Agents** (external processes running AI tasks) and user-facing clients interact with the hub over a well-defined API layer (HTTP for request/response and WebSockets for real-time updates). Key components within the Context Hub include:

* **Live CRDT Shards:** All active documents and memory objects are kept in-memory (and/or cache) as CRDTs (Conflict-Free Replicated Data Types). Each document or note is a CRDT (using a library like *Automerge* or *Yjs*) enabling concurrent edits from multiple clients. The CRDT automatically merges changes, so agent and user edits resolve without conflict. This allows real-time collaboration on shared context (multiple agents or users editing simultaneously). For example, two agents can append to a planning document at the same time; the CRDT merge ensures both contributions appear.
* **Hierarchical Folder Manager:** Documents are organized in a tree of folders (much like a file system). The folder structure helps scope the context (e.g. “Project X/Meeting Notes/…”). Each folder can contain sub-folders or documents. The folder manager maintains this hierarchy and ensures that operations like listing contents or moving a document are reflected consistently across clients. The folder structure itself may be represented as a CRDT (for concurrent edits to the structure, like two agents adding files to the same folder).
* **Index Guide Metadata:** Every folder includes a special *Index Guide* document (e.g. an `index.md` or structured metadata file). The Index Guide contains descriptive metadata and guidance about that folder’s contents. For example, it might summarize the folder’s purpose, list document types inside, or provide naming conventions. Agents consult the Index Guide to organize new content appropriately and to understand context; e.g. an Index Guide might state: “**Folder: Project X** – Contains research docs and meeting notes. Use these documents for any background info on Project X. When adding new notes, prefix with date.” This helps agents use the knowledge base correctly and consistently. The Index Guide may also include semantic pointers or tags that inform the agent how to treat documents (for instance marking certain files as “sensitive” or “final reference”). The Context Hub provides APIs to retrieve Index Guides so that agents can dynamically adjust their behavior per context.
* **Permission Manager:** All API calls and document accesses pass through a security layer enforcing per-user and per-agent permissions. Each document and folder can have an Access Control List (ACL) or sharing settings specifying which principals (users or specific agent identities) have read or write access. The permission manager checks credentials/tokens on each operation. This enables multi-user collaboration: e.g. a folder could be shared between User A and User B, allowing User B’s agents to read/write certain documents of User A. Permissions can be hierarchical (inheriting down a folder tree) and role-based (e.g. read-only vs write). Agents typically act on behalf of a user, using that user’s credentials or a delegated token with scoped access. In some cases, an agent might have its own identity if it’s a system-level agent with broader access, but even then, access can be limited to specific areas of the Context Hub. The permission model ensures that an agent for User X cannot read User Y’s private data unless a shared folder is explicitly set up.
* **Search Index:** The Context Hub maintains a search subsystem that indexes all textual content and metadata for efficient retrieval. This includes a traditional **keyword index** (for exact term or tag matching) and a **semantic vector index** (for concept-based similarity search). The search component ingests updates from the live CRDT shards – whenever a document changes, the new content is indexed. For semantic search, the system generates vector embeddings for documents (or even document chunks) using an embedding model. These embeddings are stored in a **Vector DB** (could be an in-memory index or an external service optimized for similarity search). The text index and vector index work in tandem to answer queries. For example, an agent might query for “project timeline” – the keyword index might match documents with those words, while the vector search can surface documents that are about project scheduling even if exact words differ. The result merging logic can rank results from both sources so the agent gets a comprehensive recall. The search is **scoped** by permissions and context: it will only retrieve content the requesting agent/user is allowed to see, and it can be filtered to specific folders if the agent specifies (or automatically, e.g. an agent working on Project X might limit search to that folder unless instructed otherwise).
* **Pointer Resolver:** Not all data is stored directly in CRDT documents. For binary large objects (BLOBs) like PDFs, images, audio (e.g. a recorded meeting), or large structured data (like a big JSON file or dataset), the Context Hub stores **pointers** instead of raw content. Similarly, for code artifacts or more complex knowledge bases, the Context Hub can reference external Git repositories or databases rather than store everything in the CRDT. The Pointer Resolver is responsible for managing these references. A CRDT document might contain a pointer object (e.g. a placeholder with a URI or content hash). When an agent wants to access that content, it calls a resolve API – the pointer resolver retrieves the actual binary from the blob store or fetches the latest content from the linked Git repo. This approach keeps the live context lean (no huge binaries in memory) while still integrating all data types. Pointers can include metadata like file type, size, or an *embedding of the content* (for search purposes) even if the raw data lives outside. The Context Hub may integrate with a **Blob Storage** service (for files, images, audio) and with the user’s code repositories for code-related content. For example, if an agent’s context includes “user’s codebase”, the Context Hub might just hold pointers to the user’s Git repo; the agent can ask the hub to fetch a file or commit when needed. The pointer model also means that snapshots (see below) don’t duplicate large files – they store references and perhaps version identifiers (like a blob hash or Git commit ID) to ensure reproducibility without the weight.

These components work together within the **Context Hub service**. The architecture supports both *vertical* scaling (one instance handling multiple users/agents with proper isolation) and *horizontal* scaling (multiple instances coordinating or partitioning the data, see Deployment & Scalability). The figure above illustrates how external agents interact via the API and how the internal modules connect to external storage backends.

## Storage Model and Data Organization

**Live CRDT Documents:** All dynamic data in the Context Hub is stored as **documents in a CRDT form**. We use a JSON-like CRDT structure (e.g. Automerge or Yjs document) for each note, memory item, or folder index. Each CRDT shard has a unique identifier (e.g. a UUID or compound ID like `user/folder/docname`) and can be independently synchronized. This shard-based approach means that agents only need to sync the documents they care about, not the entire database. Within a CRDT document, data can be structured (key-value, lists, text, etc.) according to needs – for example, a planning state might be a JSON object with fields, whereas a long-form note might be a rich-text sequence. The CRDT ensures **strong eventual consistency**: all clients that have the document will converge to the same state, even if they made offline or concurrent edits. Merging is handled transparently by the CRDT library. There is no need for an external lock or single writer – any number of agents and users may apply changes in parallel. The CRDT’s change history (diffs) is kept in memory while the document is active, allowing clients to sync by exchanging their latest state or operations.

**Hierarchical Folders:** Instead of one giant CRDT for everything, the Context Hub organizes documents in a hierarchy. Each folder is essentially a container that can hold subfolders or document references. The **folder listing** can itself be represented by a CRDT object (e.g. an ordered list of child items), so if two agents add files to the same folder concurrently, both file references will appear (CRDT will merge the list). Each item in a folder has metadata like a name, type (e.g. “note”, “image”, “index”, “pointer”), and an ID pointing to the actual document or pointer object. This hierarchical model allows agents to scope their operations (e.g. “list all docs in ‘ProjectX/Meetings’ folder”). It also helps with permission scoping (permissions can be set on a folder to cover all contents). The **Index Guide** in each folder (usually a document named in a special way, like `_index.guide` or similar) is stored just like any other document (a CRDT shard) but holds descriptive info rather than raw data.

**Index Guide Content and Semantics:** The Index Guide document uses a structured format (for example, Markdown with front-matter, or a JSON schema) that lists the folder’s purpose and contents. It may enumerate known document names with short descriptions, define expected file types in that folder, and provide guidance. For instance, an Index Guide might include a table: “Documents in this folder: `meeting_notes.md` (Meeting transcripts and actions), `design.doc` (System design notes)…”. It could also contain rules or hints such as “**Naming Convention:** Use `YYYY-MM-DD Title` for meeting note filenames.” Agents can parse this guide to automatically name new files correctly or decide what information to put where. Additionally, the Index Guide can have **type semantics** – e.g., marking certain documents as a “knowledge article” vs “task list”. This metadata can feed into agent prompting or behavior (for example, an agent writing code might skip documents marked as “personal journal”). In summary, the Index Guide acts like a README+manifest for each folder, improving self-organization of the knowledge base.

**External Data Pointers:** Within any document’s content, a special data structure can represent a pointer to external content. This could be a small JSON object like: `{ "type": "pointer", "subtype": "pdf", "target": "<blob_id>", "name": "Report.pdf", "preview_text": "…", "embedding": [ … ] }`. Such an object tells the agent or Context Hub that the actual content (a PDF file in this case) is stored externally (in blob storage) under some identifier. The pointer might include a short text preview or metadata (like file name, content type, maybe an embedding for search). The Context Hub stores these pointer objects inside the CRDT document so that they replicate like other data, but does not store the binary blob itself in the CRDT. When an agent needs the full content, it will call an API (e.g. `GET /pointer/<id>`) which the Pointer Resolver component handles by fetching from the blob storage or other source. For code pointers, the pointer might contain a repository URL or commit hash and path, and the Context Hub can either fetch the latest content on request or possibly keep a cached snapshot. This pointer strategy ensures the Context Hub can reference *large or changing assets (like code repos)* without ingesting them entirely. Pointers are also versioned by reference – for instance, a pointer might refer to a specific version (like a content-addressed hash or a Git commit SHA) to make snapshot recovery deterministic.

**Snapshot Layering:** While CRDT shards hold the live state, we take periodic **snapshots** of each shard’s state and push them to durable storage (like a Git repository). This snapshot process runs on a schedule (e.g. every N hours or at specific times) and also can be triggered by significant events (e.g. before an upgrade or when a document is closed after heavy editing). Snapshots are *append-only*: each snapshot is written as a new commit or version in the store, never altering history. We do not track every single CRDT operation permanently (which would be full versioning), but rather a **periodic state capture**. For example, every midnight the system might commit all changed documents to Git. The Git commit could store a serialized form of the CRDT (like a compressed JSON or binary from Automerge) for each document, or aggregate multiple documents per commit with a directory structure matching the folder hierarchy. Over time, this produces a timeline of states. If needed, an administrator can roll back a document or folder to the last snapshot (e.g. if a corruption or unwanted large-scale change happened). Snapshots also serve as an audit log: one can inspect the git history to see changes in content between snapshots. This is useful for compliance and debugging (who/what wrote something).

**CRDT State and GC:** CRDT frameworks accumulate change history (deltas) to enable peers to sync. To avoid unbounded growth, the Context Hub can **garbage-collect** CRDT history after a snapshot. Once a snapshot is taken and safely stored, the live CRDT can be reset to that baseline state (dropping old change vectors) because any peer that falls far behind can always fetch the snapshot instead of replaying a huge delta log. In practice, this might mean instantiating a fresh CRDT document from the saved state and discarding the old one’s history. This keeps memory usage and network sync costs stable. Snapshots thus act as natural compaction points. We might maintain a short window of recent changes in memory for quick undos or diagnostics, but not full multi-month history in RAM.

## API Interface (REST & WebSocket)

The Context Hub exposes a unified API for agents and user clients. The API is designed to be **language-agnostic** (HTTP+JSON for requests; WebSockets or server-sent events for pushes) so any agent (regardless of programming language) can integrate. Broadly, the API covers the following areas:

* **Document Access (REST):** Standard RESTful endpoints allow reading, creating, updating, and deleting documents. For example:

  * `GET /docs/{doc_id}` – Retrieve a document’s content (in JSON or the document’s native format). If the document is a CRDT text, it might return a serialized form or a plaintext for convenience.
  * `PUT /docs/{doc_id}` – Overwrite or create a document (for cases where a full update is needed, e.g. adding a new document with initial content).
  * `PATCH /docs/{doc_id}` – Apply a patch or diff. This can accept CRDT delta updates (if an agent has a local CRDT copy and made changes, it can sync by sending just the delta). Alternatively, a simpler patch format (like JSON Patch or a list of changes) could be accepted for agents that don’t run a CRDT client; the server will apply these to the CRDT.
  * `POST /docs` – Create a new document (with parameters like parent folder, name, type). Returns the new document ID and perhaps initial CRDT sync state.
  * `DELETE /docs/{doc_id}` – Remove a document (if permissions allow), which might just tombstone it in the CRDT or remove from folder listing.

* **Folder Navigation (REST):** Endpoints to traverse the hierarchy:

  * `GET /folders/{folder_id}` – List contents of a folder (returns metadata about subfolders and documents: names, IDs, types).
  * `POST /folders/{folder_id}` – Create a new subfolder or document within the folder (this could be an alternative to `POST /docs` with a parent).
  * `GET /folders/{folder_id}/guide` – Fetch the Index Guide document for that folder (returns its content which contains the guidance metadata). Agents may call this to understand how to use that folder.

* **Pointer Handling (REST):** Since large content is external, the API provides ways to store and retrieve via pointers:

  * `POST /blobs` – Upload a new binary blob (returns a blob ID or pointer). Agents can use this to store a file (e.g. the user dropped a PDF to be included in context). The Context Hub will forward/store it in the blob storage and return a pointer reference that the agent can then insert into a document.
  * `GET /blobs/{blob_id}` – Download a blob’s content. (This might redirect to the actual storage presigned URL, or stream it through). Agents use this when they encounter a pointer and need the full data.
  * `POST /repos/{repo_id}` – (If integrating code) perhaps trigger a fetch or update of an external Git repository’s snapshot, so the Context Hub updates its pointer to latest commit. Or create a pointer to a repository path.
  * `GET /repos/{repo_id}/{path}` – Retrieve a file from a code repository reference.

* **Search Queries (REST):** A query interface for agents to search context:

  * `GET /search?query=...&scope={folder_id}` – Perform a search over the agent’s accessible corpus. The query can be natural language or keywords. The Context Hub will run it through the hybrid search (keyword + vector) and return a list of matching documents or pointers, with relevance scores. Agents can then decide which documents to read. There may be options to filter by folder or document type. The search results might include snippets or metadata (e.g. “Document X (Meeting Notes) – contains discussion about *timeline*”).

* **Realtime Sync (WebSocket):** For agents that need continuous synchronization (especially if they maintain a local copy of some context or need immediate update notifications), the hub offers a WebSocket channel. An agent can subscribe to certain documents or folders:

  * **Document Live Updates:** Subscribe to a document’s CRDT feed. The server will stream patches over the WebSocket whenever the document changes (by any collaborator). For example, if Agent A is editing a plan and Agent B is also working on it, each has a websocket subscription and will receive CRDT ops or high-level patches as the other makes changes, in real time. The client-side CRDT can then merge these. If the agent isn’t running a CRDT library, the hub could stream the patched text or content diff instead.
  * **Endpoint:** `GET /ws/docs/{id}`. Clients connect with the document ID and receive binary patches as other collaborators edit the file.
  * **Folder Notifications:** Subscribe to a folder or query. The server can notify when new documents are added to a folder, or when any document in a folder changes (depending on interest). This is useful for agents that monitor a workspace – e.g. a summarizer agent might watch the “Meeting Notes” folder to generate a daily summary when a new note arrives.
  * **Permission/Sharing Updates:** If an agent’s access changes (e.g. user shares a new folder with the agent or revokes access), the server can push an update so the agent can adjust (perhaps disconnect from a doc it no longer should see).

The REST API uses standard authentication (likely API keys or OAuth tokens tied to a user and agent identity). The agent identifies itself and the user it’s working for, so the Context Hub can enforce permissions accordingly. The WebSocket connection would similarly be authenticated (e.g. via a token during the handshake). All responses and events include document metadata like version or CRDT vector clock, so clients can detect if they missed updates and potentially recover via a fresh `GET` or by requesting a snapshot.

In summary, the API surface allows agents to treat the Context Hub almost like a collaborative file system or database: they can read/write content, search for information, follow live changes, and store larger assets via pointers. This decouples agent logic from storage details – the agents simply express what they need (e.g. “search for X”, “update document Y with this info”) and the Context Hub handles the rest reliably.

## Folder & Index Guide Integration

The **folder structure** is critical for helping both humans and AI agents keep the context organized. The Context Hub supports a hierarchical namespace with arbitrarily nested folders (similar to directories in a file system). Each folder and document has a **unique path** (e.g. `user1/Projects/ProjectX/MeetingNotes/2025-06-01.md`), as well as a unique ID. Agents can refer to a document either by path or ID in API calls.

To ensure that agents understand how to use each folder, we leverage the **Index Guide** concept: a semi-structured document present in each folder that describes its contents. The Index Guide serves several purposes:

* **Documentation of Contents:** It can list all or important items in the folder with human-readable descriptions. This is useful for an agent or user to quickly see an overview. For example, the Index Guide might contain a bulleted list: “- **meeting\_notes/**: records of team meetings by date; - **design\_specs/**: technical design documents; - **README.md**: high-level project overview.” This acts as living documentation for the folder.
* **Organizational Conventions:** The guide can specify how new content should be structured. If a folder is meant for a certain type of data, it can tell the agent what format or naming to use. Example: “All files in this folder should be named with the pattern `YYYY-MM-DD-topic.md` and include a summary at top.” An agent creating a new document will read this and follow the convention, leading to consistency.
* **Semantic Context Cues:** The Index Guide can outline the semantic role of the folder in the user’s life or the project. For instance: “This folder contains **Project X knowledge**. Use it for any reference material or notes about Project X. Data here is considered *authoritative background* for that project.” An AI agent can use this guidance when deciding which documents might answer a query or where to record new information (knowing that if it’s about Project X, it belongs here). In essence, it’s a form of self-documenting ontology for the personal knowledge base.
* **Permissions & Sharing Notes:** The Index Guide might also include (or the system may augment it with) notes on who has access or special instructions regarding sharing. For example, if a folder is shared with a colleague or a team, the guide might say “**Shared with**: Alice, Bob – This is a collaborative space. Do not put personal notes here.” This helps agents avoid accidental information leaks or use of private info in shared spaces.

From an implementation perspective, the Index Guide is just a CRDT document that can be edited by users (and possibly by privileged agents). The system may partially generate it (e.g. listing new files automatically, similar to an auto-index) or at least update some sections, but it’s primarily human-readable and editable. Agents will typically retrieve the Index Guide when they start operating in a folder. We might provide a convenience API call like `GET /folders/{id}/guide` to fetch it directly (or it could just be one of the items in the folder listing with a known name).

The Context Hub ensures that the Index Guide always stays at top-of-list in a folder (for easy access), possibly by naming convention or a special flag in metadata. We also encourage agents to update the Index Guide when they perform significant actions, if appropriate. For example, if an agent ingests a new document into a folder, it might add an entry in the guide summarizing that document. Over time, this results in each folder having a maintained “table of contents” that improves transparency and navigability of the context.

## Permission and Sharing Model

**Multi-User and Multi-Agent Access:** The Context Hub is inherently multi-tenant – it serves multiple users, each with their own personal context, but also supports **sharing** between users in a controlled way. Each user has ownership of their data (folders and documents under their root), and by default their agents have access to that data (some agents might even be restricted to certain subfolders depending on their role – e.g. a calendar assistant agent might only need access to the “Calendar” and “Meetings” sections).

The permission model operates on two principal types: **user accounts** and **agent identities**. Every request to the Context Hub is authenticated as some user X and some agent Y (where agent Y is acting on behalf of X, or Y might be a system agent with special privileges). Permissions can then be specified in terms of users and agents. A few key aspects:

* **Per-Document/Folder ACLs:** Each document and folder can carry an ACL listing which users or agents can access it, and with what rights (read or write). By default, a document inherits permissions from its parent folder. This means an entire folder can be shared in one go by assigning an additional user to that folder’s ACL. For instance, User A can share `/Projects/ProjectX` with User B (read-write), which means User B and *User B’s agents* now have access to all docs under that folder. The system will ensure that any attempt by an unauthorized agent to read or write is rejected (HTTP 403 error).
* **Agent-Specific Restrictions:** In some cases, a user might not want all their own agents to access everything. The Context Hub can allow the user to tag an agent with a certain role or limited scope. For example, a “scheduling agent” might only be permitted on the “Calendar” folder and nothing else. Thus, even though the agent uses the user’s identity, the permission manager can consider the agent’s ID and restrict access beyond what the user generally has. This prevents a potentially untrusted or narrow-purpose agent from snooping through unrelated files. When an agent authenticates, it presents both user and agent IDs, so the hub cross-checks that agent’s allowed scope.
* **Sharing Mechanisms:** To facilitate collaboration, the system might implement sharing links or invitations. E.g., User A can invite User B to a folder; User B then gains entry (perhaps with a notification on their side). Under the hood, the folder’s ACL now includes User B with specified rights. Similarly, one could share a single document (the hub would likely treat that by sharing the containing folder or by creating a special folder just for that doc with ACL – implementation detail). The permission manager ensures that if User B is removed, their access is immediately revoked (and any active agent connections for B to that content are terminated via the WebSocket channel).
* **Auditing and Logs:** Every change made to the Context Hub (document edits, permission changes, share events) is logged with the initiating user/agent identity and timestamp. This provides an audit trail, complementing the snapshot history. If there’s a misuse or data leak, admins or users can trace which agent was responsible. This ties into the Policy/Firewall aspect of the OS, providing transparency.

**Concurrency and Conflict Resolution:** Because multiple users or agents might truly edit *the same document* at the same time (especially in a shared space), the CRDT approach handles the low-level merge. But at a higher level, the permission model might want to prevent certain overwrites – for example, if a user with read-only access somehow tries to send an edit, it should be rejected before even hitting the CRDT. In shared scenarios, all collaborators with write access are effectively equal peers (there is no locking; last-write-wins via CRDT merge). However, we may apply some policy for certain fields (for example, if two agents try to change a task status field, we might have a business rule to avoid flip-flopping – those would be handled at the application layer on top of CRDT’s outcome).

In summary, the permission model ensures **isolation by default** (users see only their own data) and **controlled sharing** when desired, with the flexibility to restrict even a user’s own agents. This is crucial in a system where autonomous agents are acting on user’s behalf – the user remains in control of what each agent can touch.

## Search and Indexing Layer

To make vast amounts of context data useful, the Context Hub provides a powerful search and retrieval system. The search capability is **hybrid** – combining keyword-based lookup with semantic similarity – to accommodate both precise queries (like “find document titled Budget Q1”) and fuzzy queries (like “find information about our hiring plans” which might not use exact wording). Key points of the search design:

* **Full-Text Index:** All textual content in the live documents (CRDT shards) and perhaps important metadata from pointers is indexed in a full-text search engine. This could be an embedded engine or a service (like ElasticSearch or Meilisearch, depending on scale). Each document is indexed with its content, title, path, and tags (including folder names, document type tags, etc., which improves recall). The index is updated **incrementally**: when a document changes, the relevant index entry is updated. We use a background job or the live update feed to trigger re-indexing of documents upon edits (with some debounce to avoid doing it on every single keystroke-sized change).
* **Semantic Vector Index:** In parallel, an embedding model (like a transformer-based sentence encoder) generates vector representations of documents or even sections of documents. For example, each note or each paragraph might have an embedding. These vectors are stored in a specialized vector database or library that supports similarity search (e.g. FAISS, Pinecone, or an integrated vector index in our search engine). The vector index is also maintained incrementally – whenever content is added or significantly changed, new embeddings are computed (possibly by an asynchronous worker to avoid slowing down the main operation) and upserted into the vector DB. Each vector entry is linked to the document (and position within it, if we index chunks).
* **Unified Query Processing:** When an agent issues a search query, the Context Hub processes it through both mechanisms. A keyword search will retrieve documents containing the terms (or fuzzy matches, etc.), yielding a set of candidate documents with relevance scores. Simultaneously (or for longer/natural language queries), we embed the query using the same embedding model and query the vector index for nearest vectors. This yields another set of candidates with similarity scores. The search service then merges the results. One approach is to take the union of candidates and then sort by some combined relevance (possibly normalizing scores from both sources). The merged results are then filtered by permissions (ensuring no forbidden docs slip through) and by any scope constraints (if the query was limited to a folder or tag). The final list returned to the agent includes document references, a brief snippet or summary, and a score. Agents can use this to decide which documents to read or present to the user.
* **Scoped and Advanced Search:** Agents might specify in queries to only search certain domains (like “only search my emails for X”). The Context Hub supports such filters by restricting the search to specific folders or document types. The folder structure and Index Guides aid this: e.g. an Index Guide might tag that “emails” folder with a type, so the search index knows that all docs under it are emails. Then a query filter `type:email` could be applied. Additionally, the user’s context (like active project) might be used to implicitly boost or filter results.
* **Indexing Snapshots:** Primarily, search focuses on live data (current state of CRDT docs). However, the design calls for searching **across both live shards and snapshot data**. This means if a document was deleted or significantly changed, an older snapshot might hold information that’s no longer in the live view. For comprehensive recall (especially for audit or memory retrieval purposes), the Context Hub search can optionally include snapshot index. One way to achieve this is by not deleting index entries for removed content but rather marking them with a historical flag. Alternatively, we maintain a separate index of snapshot archives (perhaps with lower ranking or only accessed when explicitly asked for historical data). For example, an agent could specify `history:true` in a query to include past states. This ensures that knowledge isn’t lost just because it was deleted or edited in the live context – agents can still find that older info if needed (assuming permissions allow access to historical data – we might restrict this to the owning user or admin-level queries).
* **Continuous Improvement:** The search system can gather feedback signals (like which result an agent actually read or found useful) to improve ranking over time. And because new data sources might get integrated (emails, browsing history, etc.), the indexing pipeline is designed to be extensible: connectors can feed new documents or update existing ones as those external sources change.

The outcome is a robust **Contextual Retrieval** capability, essential for agents to locate the right piece of information in a large personal knowledge base. An agent can rely on the Context Hub to do heavy-lifting for memory queries, which also simplifies agent prompt sizes (they don’t need to carry all relevant info if they can quickly fetch it when needed).

## Snapshotting and Lifecycle Management

Regular snapshotting provides durability and an audit trail for the Context Hub’s data. Here’s how the snapshot system works and fits into the overall lifecycle of data:

* **Periodic Snapshots:** On a configurable schedule (say, every 24 hours at midnight, or every X hours), the Snapshot Manager component takes a point-in-time snapshot of all or changed CRDT shards. In a simple implementation, it could iterate over each document’s state and save it. More efficiently, it can track which documents have un-snapshotted changes since the last snapshot and only flush those. Each snapshot operation yields an immutable record in the snapshot store. We propose using **Git** as the snapshot store due to its append-only, versioned nature and widespread tooling. For each snapshot cycle, the Context Hub could commit a new Git branch or tag (for a full backup) or commit to a branch representing that document’s history. For example, each document might correspond to a file in the repo, and we commit all files (or a bundle) at once – the commit ID and timestamp serves as the snapshot identifier.
* **Trigger-based Snapshots:** In addition to timed snapshots, certain events trigger snapshots: e.g. a user manually triggers one before a risky operation, or the system triggers on significant changes (if a document becomes very large after an import, we might snapshot immediately to reduce risk of losing it). Also, upon shutdown of the Context Hub service or before upgrade, a snapshot ensures state is safely stored.
* **Snapshot Content:** What exactly is stored? For each CRDT document, we store a serialized state (for Automerge, this might be its binary or a JSON dump; for Yjs, a update message or state vector). We also store the Index Guides and folder structure (which might just be special documents anyway). For pointers, we typically don’t snapshot the external content (since those live in their own storage), but we do snapshot the pointer metadata (so that if you roll back, you still reference the same external blob ID or commit hash as before, ensuring consistency). It’s important that snapshots are **self-contained enough** that one could reconstruct the entire Context Hub state from scratch: that means including all documents and folder structures. Because the blob storage is external, as long as the pointers (which include blob IDs or hashes) are intact, the external files can be fetched – we assume the external stores themselves are durable (for true disaster recovery, external content should ideally be in durable cloud storage with its own backups).
* **Retention and Pruning:** Snapshots being append-only means we keep an ever-growing history. This is great for audit but could become heavy. We likely implement a policy to **prune or compress old snapshots** over time. For example, keep daily snapshots for last 30 days, weekly for last 6 months, monthly beyond that, etc. If using Git, the history is incremental deltas, which is storage-efficient for text. We can also archive old snapshot data to cold storage if needed. However, the append-only principle means we never mutate past data; pruning would be an explicit archival rather than in-place deletion.
* **Recovery and Rollback:** In the event of a catastrophic failure (server crash with memory loss) or data corruption, the latest snapshot can be used to restore the Context Hub. The process would be: load the snapshot state for each document into a fresh CRDT instance, then resume normal operations (perhaps also replaying any logged operations since the snapshot if we have a WAL – we might log live ops in between snapshots for near-real-time durability, in addition to periodic full snapshots). Rollback to a previous snapshot (for instance, if an agent erroneously deleted a bunch of content and we want to undo that beyond just what CRDT history can do) would involve taking the snapshot from before the event and restoring it as the current state. This would overwrite the live state of those documents/folders with the old snapshot. Because not everything is fully versioned, granular rollback of a specific edit might not be possible, but coarse rollback to an earlier day’s state is. After a rollback, a new snapshot would be taken (so that the rollback itself is recorded).
* **Garbage Collection of CRDT Ops:** As mentioned in Storage Model, after a snapshot, we can drop old CRDT operations from memory (since the snapshot acts as a new baseline). This is analogous to clearing the diff log once changes are persisted. If using Automerge, for example, we could instantiate a new Automerge doc from the JSON snapshot – that doc’s internal op log is now minimal. This keeps the live collaboration efficient. The system might do this GC step in the background, one document at a time, to avoid pause. During GC of a particular doc, we might temporarily pause edits or queue them, but since snapshots are periodic, this is manageable.
* **Consistency and Shapshots of Multiple Docs:** One consideration is whether snapshots across documents need to be consistent with each other (a single coherent snapshot of the whole system). Using Git and committing all docs together gives a consistent cut of the entire context at that time. This is ideal for disaster recovery (you can restore everything to exactly how it was at midnight). It’s possible that in between, some cross-document references (like a link from one doc to another) might get slightly out of sync if not snapshot at the exact same moment, but since we snapshot everything together, we avoid that. In highly concurrent systems, to snapshot a point-in-time, we might quiesce writes for a few seconds or use copy-on-write snapshots. In practice, locking all writes for the second it takes to capture state might be acceptable during off-hours. If that’s not acceptable, we could take snapshots per document at different times – simpler but then cross-document consistency isn’t guaranteed. The design leans towards a unified snapshot for simplicity and integrity.

**Operational Lifecycle:** Summarizing the lifecycle of context data: Documents are created and edited freely (with CRDT capturing changes), periodically these changes are consolidated into a snapshot which trims the live history. The snapshot history accumulates, enabling look-back and system recovery. If the Context Hub service restarts, it bootstraps by loading the last snapshot of each needed document (on-demand – e.g. lazy load when a document or folder is accessed, to avoid loading everything). Agents reconnect and resync via CRDT (if an agent was offline, it might use the snapshot as a baseline and then get deltas that happened after it via the WebSocket feed or a smaller log). This design ensures that even if agents or the hub go offline, they can catch up from the durable state and continue seamlessly.

## Deployment and Scalability Considerations

The Context Hub is designed to scale with the number of users, agents, and volume of data, while remaining performant for real-time operations:

* **Stateless API Frontends:** The REST/WS API layer can be run on multiple instances behind a load balancer for horizontal scale. These frontends handle authentication, permission checking, and routing of requests to the appropriate backend component. They can be stateless (no local data, just ephemeral), meaning any instance can serve any request after auth. WebSocket connections might stick to one server (sticky session) for efficiency, but the system can handle many such connections by distributing users across instances.

* **Stateful Collaboration Backend:** The CRDT shard management can be partitioned. For example, we might shard by user or by document type. If a particular user or project has a very active set of documents, those could be handled by a specific node (or set of nodes) to keep CRDT synchronization local. Technologies like Yjs can function in a distributed manner with WebRTC or a central signaling server – but for our design, we likely implement the CRDT merge on the server side. One deployment approach is to have a **cluster of Context Hub nodes** that share a database or use a pub-sub to propagate CRDT ops among them. Alternatively, use a consistent hashing or directory service to assign each document to a particular node (e.g. doc ID’s hash -> node). The API layer then routes operations for that doc to the correct node. This prevents two different servers from separately maintaining the same document state and drifting.

* **Data Storage Scaling:** The snapshot store being Git could become large, but Git is efficient with text diffs. For many users, a single monorepo might not scale well (lots of files). We could choose to have per-user snapshot repositories or partitions by project. Similarly, blob storage for attachments should be on a scalable system (like S3 or a distributed file store). The vector search can be scaled by sharding the vector index (e.g. by using a distributed vector DB or splitting index by data domain).

* **Performance:** Real-time editing performance is crucial. CRDTs like Automerge have some memory overhead, but by segmenting data into documents, we limit how much needs to be in memory and updated for any given interaction. The system should handle documents with frequent updates (like a chat log or collaborative note) in memory easily. For rarely used documents, we might *swap them out*: i.e. only load into memory when accessed. This is akin to how an OS pages memory. If a doc hasn’t been touched since last snapshot, we can just keep its snapshot on disk and not maintain a live CRDT until needed. This way, memory usage scales with active working set, not total documents.

* **Concurrent Edit Scaling:** Using CRDT means most of the heavy lifting (merging) is done client-side or server-side in-memory without needing central transaction locks. However, if a *lot* of concurrent operations flood in (e.g. 100 agents all appending to the same document at once), we need to ensure the server can handle the merge throughput. Automerge and Yjs are typically efficient for moderate concurrency (tens of edits per second). For extremely high rates, we might batch operations or apply rate-limiting at the API (which is unlikely in typical use cases – 100 autonomous agents all writing simultaneously to one user’s note is an edge case). In multi-user collaboration, the number of concurrent editors on one doc is probably small (a few people/agents).

* **Network and Latency:** Agents may run anywhere (cloud, user’s device). The hub being cloud-based (likely) means network latency for each operation. To mitigate this, the WebSocket live sync helps by pushing changes proactively (no need for agents to poll). For read-heavy scenarios, an agent might cache some data. We could also consider edge caches for frequently read public or shared documents (though most data is private). The search queries might be heavier, but those can be handled by the search engine asynchronously.

* **Scaling Team/Org Scenarios:** If Lightning OS expands to small teams or organizations (multi-user collab beyond just personal), the Context Hub could face hundreds of users. We’d possibly deploy a multi-node cluster: one node could act as the coordinator (for auth, search queries global), while multiple worker nodes handle sets of folders/projects. A directory service (or a distributed database) would keep track of which node has which document loaded. Alternatively, every node could potentially handle any doc by loading it on demand from a snapshot – but then concurrency control is needed to avoid two nodes diverging. A simple strategy is to assign a **primary node** for each shard responsible for merging ops, and other nodes (if any) subscribe for read-only caches. Given CRDT doesn’t need a single writer, it’s feasible to have a peer-to-peer sync model between nodes, but that adds complexity. For now, a master-worker approach per document could be sufficient: one node merges and broadcasts official changes to others.

* **Fault Tolerance:** If a node fails, its load can be taken over by another node by loading snapshots. WebSocket clients would reconnect and resync (the CRDT can reconcile any missed operations via the snapshot baseline + new ops from others). Having snapshots in a shared store (Git or DB) accessible by all nodes ensures no single point of failure on data. We should also store the latest CRDT states (or at least the ops not yet snapshotted) in a replicated in-memory store or write-ahead log to avoid losing the last few changes if a node crashes between snapshots. Perhaps a lightweight journal of ops could be appended to something like Redis or Kafka for durability – which could also double as the pub-sub for multi-node sync.

* **Deployment Environment:** The Context Hub likely runs as part of the Lightning OS cloud backend. Each user could be namespaced for security. Containers or microservices separate major functions: e.g. a service for the API & permission checks, a service for search indexing and querying, a service for snapshot management (could even be a separate process that just does git commits asynchronously), and perhaps a service managing blob storage (or relying on cloud storage directly). This separation allows scaling each component based on load (e.g. search might require more CPU for embedding calculations, so scale that out; the core CRDT service might need more memory, so scale accordingly). Using Kubernetes or similar orchestration, we ensure that even if pods restart, they reload from snapshots and continue operation with minimal downtime.

  The reference implementation exposes configuration via environment variables (`HOST`, `PORT`, `DATA_DIR`, `SNAPSHOT_DIR`, `INDEX_DIR`, and `BLOB_DIR`) so deployments can easily override paths and network settings. A Dockerfile is included for containerized runs.

In conclusion, the refined Context Hub design provides a **robust, collaborative, and extensible** foundation for agent memory and personal data. It marries cutting-edge real-time collaboration tech (CRDTs) with reliable backend practices (snapshots, ACL security, search indexing) to meet the requirements of Lightning OS. This design will enable agents to seamlessly share context, recover from errors, and respect user privacy – all while scaling to support many users and a rich variety of data in the system.

**Sources:** Lightning OS internal specifications and architecture principles derived from system design best practices.

